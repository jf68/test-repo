Contents
1.	What is a support vector machine? What is a maximal margin classifier? What is a support vector classifier? How is SVM similar to logistic regression and linear discriminant analysis?	1
2.	What is a support vector classifier?	1
3.	How is a SVC similar or different from logistic regression and linear discriminant analysis?	2
4.	What is a Support Vector Machine?	2
5.	Why can Logistic Regression be bad? Why would we need Linear Discriminant Analysis?	2
6.	What is Linear Discriminant Analysis? How is it different from Logistic Regression?	2
7.	What is Quadratic Discriminant Analysis? Compare it to LDA.	3
8.	What is Random Forest imputation?	3
9.	What is AUC/ROC?	3
10.	Discuss Cross Validation. What is it? Why might k-fold CV be better than Leave-One-Out-CV?	3



1.	What is a support vector machine? What is a maximal margin classifier? What is a support vector classifier? How is SVM similar to logistic regression and linear discriminant analysis?
•	A maximal margin classifier is a classifier that assumes the data is linearly separable
•	Margin means the smallest distances between each training point and the separating hyperplane
•	The maximal margin is where the hyperplane has the farthest minimum distance to each training observation
•	For large p, it can lead to overfitting
•	‘Support vectors’ are the vectors in p-dim. Space where if they were moved, then the maximal margin hyperplane would shift, the hyperplane is completely dependent on this set of vectors
2.	What is a support vector classifier?
•	Support vector classifier is a soft margin classifier – some of the observations are classified on the wrong side of the margin in hopes of classifying the majority of the observations correctly, for data that is not linearly separable
•	C is the budget for the amount of margin that can be violated by the observations, use cross validation to determine best C
•	Support vectors – observations that lie directly on the margin or on the wrong side of the margin determine the hyperplane (not the observations that lie on the correct side of the margin)
•	Large C will have high bias-low variance, low C will have low bias-high variance, C = 0 is the maximal margin classifier
3.	How is a SVC similar or different from logistic regression and linear discriminant analysis?
•	LDA – classifies based on mean of all observations within each class, and within-class covariance matrix using all observations
•	Logistic regression has very low sensitivity to observations far from the decision boundary
•	For classes that are well separated, SVM will perform better
•	For observations that overlap, logistic regression is preferred
•	SVC optimization problem can be rewritten as a logistic regression minimization problem with the penalty term included (in regularization)
4.	What is a Support Vector Machine?
•	SVM has low sensitivity to observations on the correct side of the margins and far from the classifier
•	SVM is a classifier that is a function of a kernel, which can be linear, polynomial, or radial, which in turn is a function of the support vectors
5.	Why can Logistic Regression be bad? Why would we need Linear Discriminant Analysis?
•	When classes are well-separated, parameter estimates are very unstable.
•	Small sample size and distribution of predictors is approximately normal
•	Only good for binary classification
•	LDA does not suffer from these problems
6.	What is Linear Discriminant Analysis? How is it different from Logistic Regression?
•	We can make an estimate of the probability of predictors and which class the response falls in each class. However, we have to invoke Bayes’ Theorem. As a result, need to estimate the parameters assuming each predictor follows Gaussian Distribution.
•	LDA estimates Gaussian parameters and probabilities using training data set
•	Assumptions: for observations in each class, they have a common class-mean and common variance across all classes and they follow Gaussian distribution
•	LDA provides an estimate of the naïve Bayes estimation
•	Logistic regression estimates the coefficients for log odds using MLE, LDA estimates it as functions of the Gaussian distribution mean and variance
•	Logistic regression is better when the training set does not follow Gaussian distribution

7.	What is Quadratic Discriminant Analysis? Compare it to LDA.
•	Similar to LDA but assumes that each class has a common variance as to a common variance across the entire data
•	LDA tends to have a lower variance in bias-variance tradeoff as opposed to QDA since it is much less flexible in its estimation of the parameter estimates
•	If there isn’t a common covariance matrix across classes, then LDA becomes a horrible classifier with high bias
•	If there are few training observations, then LDA is preferred to reduce variance
•	If training set is large or there is clearly different class covariances, then QDA is preferred.
•	QDA is a compromise between the non-parametric KNN and the LDA for non-linear classification
8.	What is Random Forest imputation?
•	First replace missing values by average value
•	Then iterate through by using imputed values calculated so far to train a random forest
•	Compute proximity matrix, use the proximity as weight to impute the missing values as the weighted average of non-missing values
•	Proximity matrix – measure of similarity/dissimilarity between pairs of variables or ‘distance’

9.	What is AUC/ROC?
•	

10.	Discuss Cross Validation. What is it? Why might k-fold CV be better than Leave-One-Out-CV?

•	Divide your dataset into k subsets. Take k-1 subsets and use these subsets together as a training set to fit a model.
•	Then estimate the MSE or Error rate in classification of the kth subset, which is treated as a test set. Pick the model with the lowest error rate as your final model
•	LOOCV has low bias but high variance, and high computational prerequisites
•	LOOCV has high variance because with each model, it is almost always trained on the same subset whereas the k-fold models have much fewer overlap
•	LOOCV does have nearly unbiased estimates because it uses all the data
•	K=5 or k=10 has been shown to give a generally low test error rate

11.	What is hierarchical clustering?

